# Variational Auto Encoder
VAEs are generative models based on the autoencoder framework. They rely on an encoder and decoder to map inputs, $X$, into a parameterised latent representation, $z$ , then map from this latent space back to the input space (to regenerate the input) respectively. New data can be generated by running a random vector in the latent space through the decoder. Models implented here use a Gaussian variational approximation for the latent space. 

## Models
1. Vanilla VAE - basic variational autoencoder with encoder/decoder structure based on PyTorch [examples](https://github.com/pytorch/examples). The encoder models $Q(z|X)$ and the decoder models $P(X|z)$.
2. Conditional VAE - model incorporates class label, $c$, to allow generation of a new image with a chosen label. The encoder now models $Q(z|X, c)$ and the decoder models$P(X|z, c)$.